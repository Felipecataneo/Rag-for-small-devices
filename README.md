
# Raspberry RAG: Um Pipeline de RAG Otimizado para Raspberry Pi

Este projeto implementa um pipeline completo de **Retrieval-Augmented Generation (RAG)** projetado para rodar de forma eficiente em hardware com recursos limitados, como um Raspberry Pi. Ele permite que voc√™ fa√ßa perguntas em linguagem natural sobre uma cole√ß√£o de documentos PDF e receba respostas geradas por um modelo de linguagem local (LLM) rodando via Ollama.

O sistema √© composto por duas partes principais:
1.  **Processador de PDFs (`pdf_processor.py`):** Uma ferramenta de linha de comando que l√™ uma pasta de documentos PDF, os divide em peda√ßos, gera embeddings vetoriais e os armazena em um banco de dados vetorial customizado e leve.
2.  **API de Busca e RAG (`vector_api.py`):** Um servidor FastAPI que exp√µe endpoints para realizar buscas vetoriais e um pipeline RAG completo, desde a consulta at√© a gera√ß√£o da resposta em streaming.

## ‚ú® Funcionalidades

-   **Processamento de PDFs:** Extrai texto de m√∫ltiplos arquivos PDF e os divide em peda√ßos (chunks) gerenci√°veis.
-   **Embeddings Locais:** Utiliza `sentence-transformers` para gerar embeddings vetoriais localmente, sem depender de APIs externas.
-   **Banco de Dados Vetorial Leve:** Implementa um `HierarchicalVectorTape`, um sistema de armazenamento customizado que combina:
    -   **Filtro de Bloom:** Para uma pr√©-filtragem extremamente r√°pida de documentos relevantes.
    -   **Quantiza√ß√£o de Vetores:** Para reduzir o uso de disco e mem√≥ria.
    -   **Arquivos Mapeados em Mem√≥ria:** Para buscas eficientes com baixo overhead.
-   **API com FastAPI:** Oferece uma API robusta e ass√≠ncrona para consultas.
-   **Integra√ß√£o com Ollama:** Conecta-se a um servidor Ollama local para gera√ß√£o de texto, permitindo o uso de diversos LLMs open-source (Llama3, Phi-3, etc.).
-   **Streaming de Respostas:** As respostas do LLM s√£o transmitidas em tempo real (Server-Sent Events), proporcionando uma experi√™ncia de usu√°rio mais interativa e responsiva.
-   **Otimizado para Edge/IoT:** Projetado com baixo consumo de recursos para rodar em dispositivos como o Raspberry Pi.

## üèõÔ∏è Arquitetura

  <!-- Voc√™ pode criar um diagrama e subir no imgur ou similar para colocar aqui -->

1.  **Indexa√ß√£o (Offline):**
    -   O `pdf_processor.py` √© executado.
    -   Ele l√™ os PDFs da pasta `/pdfs`.
    -   Os textos s√£o divididos e tokenizados.
    -   O modelo `all-MiniLM-L6-v2` gera embeddings para cada peda√ßo de texto.
    -   Os tokens s√£o adicionados a um Filtro de Bloom, e os vetores (quantizados) s√£o salvos em um arquivo "tape". Metadados s√£o salvos separadamente.
2.  **Consulta (Online):**
    -   O usu√°rio envia uma `query` para o endpoint `/rag/query` da API.
    -   A API gera o embedding da query.
    -   O Filtro de Bloom rapidamente descarta documentos que n√£o cont√™m os tokens da query.
    -   Uma busca por similaridade de cosseno √© realizada nos vetores candidatos.
    -   Os `top-k` trechos de texto mais relevantes s√£o recuperados.
    -   Um prompt √© constru√≠do, contendo o contexto recuperado e a pergunta original do usu√°rio.
    -   O prompt √© enviado para o modelo LLM rodando no Ollama.
    -   A API transmite a resposta do LLM de volta para o cliente, token por token.

## üöÄ Como Usar

### Pr√©-requisitos

-   Python 3.9+
-   [Ollama](https://ollama.com/) instalado e rodando.
-   Um modelo LLM baixado via Ollama (ex: `ollama pull phi3:mini`).
-   Um ambiente virtual Python (recomendado).

### 1. Instala√ß√£o

Primeiro, clone o reposit√≥rio:
```bash
git clone https://github.com/seu-usuario/raspberry-rag.git
cd raspberry-rag
```

Crie e ative um ambiente virtual:
```bash
python3 -m venv .venv
source .venv/bin/activate
```

Instale as depend√™ncias:
> **Nota para Raspberry Pi (ARM):** O PyTorch (`torch`) precisa ser instalado manualmente primeiro. Consulte o [guia oficial do PyTorch](https://pytorch.org/) para a wheel correta para sua arquitetura. Ap√≥s instalar o torch, o restante pode ser instalado via `requirements.txt`.

```bash
pip install -r requirements.txt
```

### 2. Adicione seus Documentos

Coloque todos os seus arquivos `.pdf` dentro da pasta `/pdfs`.

### 3. Indexe os Documentos

Execute o script de processamento para criar o banco de dados vetorial. Isso s√≥ precisa ser feito uma vez ou sempre que voc√™ adicionar/modificar seus PDFs.

```bash
python pdf_processor.py
```
Ao final, uma pasta `/vector_db` ser√° criada com os dados indexados.

### 4. Inicie a API

Inicie o servidor FastAPI. Voc√™ pode configurar o modelo Ollama a ser usado atrav√©s de uma vari√°vel de ambiente.

```bash
# Exemplo usando o modelo 'phi3:mini'
export OLLAMA_MODEL="phi3:mini"

# Inicia o servidor
python vector_api.py
```
A API estar√° rodando em `http://localhost:8000`.

### 5. Fa√ßa uma Consulta

Voc√™ pode usar `curl` ou qualquer cliente de API para fazer uma pergunta.

```bash
curl -X POST "http://localhost:8000/rag/query" \
     -H "Content-Type: application/json" \
     -d '{"query": "Fa√ßa um resumo dos pdfs"}'
```

A resposta ser√° transmitida em formato Server-Sent Events (SSE):
```
data: {"type": "sources", "data": [{"source": "A_tutorial_on_stochastic_programming.pdf", "chunk_id": 2, "similarity": 0.2173}]}

data: {"type": "llm_token", "data": "A"}

data: {"type": "llm_token", "data": " principal"}

data: {"type": "llm_token", "data": " conclus\u00e3o"}

data: {"type": "llm_token", "data": " do"}

data: {"type": "llm_token", "data": " paper"}

...
```

## üõ†Ô∏è Configura√ß√£o e Implanta√ß√£o

### Vari√°veis de Ambiente

O comportamento da API pode ser controlado por vari√°veis de ambiente:
-   `OLLAMA_MODEL`: Nome do modelo a ser usado pelo Ollama (padr√£o: `gemma3:1b`).
-   `OLLAMA_HOST`: URL do servidor Ollama (padr√£o: `http://localhost:11434`).
-   `HOST`: Host em que a API ir√° escutar (padr√£o: `0.0.0.0`).
-   `PORT`: Porta da API (padr√£o: `8000`).

### Implanta√ß√£o no Raspberry Pi

1.  **Instale o Ollama** no Pi.
2.  **Clone o reposit√≥rio** e instale as depend√™ncias (lembre-se de instalar o `torch` espec√≠fico para ARM).
3.  **Recrie o banco de dados** executando `pdf_processor.py` diretamente no Pi, pois os arquivos bin√°rios podem n√£o ser port√°teis entre arquiteturas.
4.  **Inicie a API** usando um script ou servi√ßo (veja `start_api.sh` como exemplo).
5.  **(Opcional) Exponha para a Internet** usando um servi√ßo de t√∫nel como o [Ngrok](https://ngrok.com/): `ngrok http 8000`.

## ü§ù Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir uma issue ou enviar um pull request para melhorias, corre√ß√µes de bugs ou novas funcionalidades.

## üìÑ Licen√ßa

Este projeto √© distribu√≠do sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.
```